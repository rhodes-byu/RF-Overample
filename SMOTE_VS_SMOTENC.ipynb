{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from SupportFunctions.load_datasets import load_datasets\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artificial_tree.csv (5000 rows, 41 columns)\n",
      "Loaded: audiology.csv (190 rows, 68 columns)\n",
      "Loaded: balance_scale.csv (625 rows, 5 columns)\n",
      "Loaded: breast_cancer.csv (699 rows, 10 columns)\n",
      "Loaded: car.csv (1728 rows, 7 columns)\n",
      "Loaded: chess.csv (3196 rows, 37 columns)\n",
      "Loaded: crx.csv (664 rows, 15 columns)\n",
      "Loaded: diabetes.csv (768 rows, 9 columns)\n",
      "Loaded: ecoli_5.csv (327 rows, 8 columns)\n",
      "Loaded: flare1.csv (323 rows, 11 columns)\n",
      "Loaded: glass.csv (214 rows, 10 columns)\n",
      "Loaded: heart_disease.csv (303 rows, 14 columns)\n",
      "Loaded: heart_failure.csv (299 rows, 11 columns)\n",
      "Loaded: hepatitis.csv (138 rows, 16 columns)\n",
      "Loaded: hill_valley.csv (606 rows, 101 columns)\n",
      "Loaded: ionosphere.csv (351 rows, 35 columns)\n",
      "Loaded: iris.csv (150 rows, 5 columns)\n",
      "Loaded: lymphography.csv (148 rows, 19 columns)\n",
      "Loaded: mnist_test.csv (10000 rows, 785 columns)\n",
      "Loaded: optdigits.csv (3823 rows, 65 columns)\n",
      "Loaded: parkinsons.csv (195 rows, 23 columns)\n",
      "Loaded: seeds.csv (199 rows, 8 columns)\n",
      "Loaded: segmentation.csv (211 rows, 20 columns)\n",
      "Loaded: sonar.csv (208 rows, 61 columns)\n",
      "Loaded: tic-tac-toe.csv (958 rows, 10 columns)\n",
      "Loaded: titanic.csv (712 rows, 8 columns)\n",
      "Loaded: treeData.csv (1440 rows, 61 columns)\n",
      "Loaded: waveform.csv (5000 rows, 41 columns)\n",
      "Loaded: wine.csv (178 rows, 14 columns)\n",
      "Loaded: zoo.csv (101 rows, 18 columns)\n",
      "\n",
      "Total datasets loaded successfully: 30\n"
     ]
    }
   ],
   "source": [
    "all_datasets = load_datasets()\n",
    "selected = [\"sonar\", \"crx\", \"titanic\", \"ionosphere\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing dataset: tic-tac-toe\n",
      "\n",
      "        win TL TM TR ML MM MR BL BM BR\n",
      "0  positive  x  x  x  x  o  o  x  o  o\n",
      "1  positive  x  x  x  x  o  o  o  x  o\n",
      "2  positive  x  x  x  x  o  o  o  o  x\n",
      "3  positive  x  x  x  x  o  o  o  b  b\n",
      "4  positive  x  x  x  x  o  o  b  o  b\n",
      "\n",
      "Column Data Types:\n",
      "win    object\n",
      "TL     object\n",
      "TM     object\n",
      "TR     object\n",
      "ML     object\n",
      "MM     object\n",
      "MR     object\n",
      "BL     object\n",
      "BM     object\n",
      "BR     object\n",
      "dtype: object\n",
      "\n",
      "Categorical columns detected based on dtype:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>win</th>\n",
       "      <th>TL</th>\n",
       "      <th>TM</th>\n",
       "      <th>TR</th>\n",
       "      <th>ML</th>\n",
       "      <th>MM</th>\n",
       "      <th>MR</th>\n",
       "      <th>BL</th>\n",
       "      <th>BM</th>\n",
       "      <th>BR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>negative</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>negative</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>negative</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>negative</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>negative</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>958 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          win TL TM TR ML MM MR BL BM BR\n",
       "0    positive  x  x  x  x  o  o  x  o  o\n",
       "1    positive  x  x  x  x  o  o  o  x  o\n",
       "2    positive  x  x  x  x  o  o  o  o  x\n",
       "3    positive  x  x  x  x  o  o  o  b  b\n",
       "4    positive  x  x  x  x  o  o  b  o  b\n",
       "..        ... .. .. .. .. .. .. .. .. ..\n",
       "953  negative  o  x  x  x  o  o  o  x  x\n",
       "954  negative  o  x  o  x  x  o  x  o  x\n",
       "955  negative  o  x  o  x  o  x  x  o  x\n",
       "956  negative  o  x  o  o  x  x  x  o  x\n",
       "957  negative  o  o  x  x  x  o  o  x  x\n",
       "\n",
       "[958 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview dataset\n",
    "dataset_name = \"tic-tac-toe\"\n",
    "df = all_datasets[dataset_name]\n",
    "\n",
    "print(f\"Previewing dataset: {dataset_name}\\n\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "cat_cols = list(df.select_dtypes(include=['object', 'category']).columns)\n",
    "print(\"\\nCategorical columns detected based on dtype:\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating with encoding technique: onehot\n",
      "==================================================\n",
      "Processing dataset: crx\n",
      "==================================================\n",
      "Dataset: crx - Avg Weighted F1 (SMOTE onehot): 0.8753\n",
      "Dataset: crx - Avg Weighted F1 (SMOTENC): 0.8797\n",
      "==================================================\n",
      "Processing dataset: flare1\n",
      "==================================================\n",
      "Dataset: flare1 - Avg Weighted F1 (SMOTE onehot): 0.7897\n",
      "Dataset: flare1 - Avg Weighted F1 (SMOTENC): 0.7460\n",
      "==================================================\n",
      "Processing dataset: titanic\n",
      "==================================================\n",
      "Dataset: titanic - Avg Weighted F1 (SMOTE onehot): 0.7859\n",
      "Dataset: titanic - Avg Weighted F1 (SMOTENC): 0.7906\n",
      "\n",
      "Evaluating with encoding technique: ordinal\n",
      "==================================================\n",
      "Processing dataset: crx\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 117\u001b[0m\n\u001b[0;32m    113\u001b[0m rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m    114\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m    115\u001b[0m     X, target, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mrs\n\u001b[0;32m    116\u001b[0m )\n\u001b[1;32m--> 117\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models_on_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_technique\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scores[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m scores[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     results_f1[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmote\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(scores[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 66\u001b[0m, in \u001b[0;36mevaluate_models_on_split\u001b[1;34m(X_train, X_test, y_train, y_test, encoding_technique, random_state)\u001b[0m\n\u001b[0;32m     64\u001b[0m clf_smote \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m     65\u001b[0m clf_smote\u001b[38;5;241m.\u001b[39mfit(X_train_res, y_train_res)\n\u001b[1;32m---> 66\u001b[0m y_pred_smote \u001b[38;5;241m=\u001b[39m \u001b[43mclf_smote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m f1_smote \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_pred_smote, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# --- SMOTENC ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate_models_on_split(X_train, X_test, y_train, y_test, encoding_technique='onehot', random_state=42):\n",
    "    min_count = min(Counter(y_train).values())\n",
    "    if min_count < 2:\n",
    "        print(\"Skipping iteration: Not enough minority samples for SMOTE.\")\n",
    "        return None, None\n",
    "\n",
    "    k_neighbors = min(5, min_count - 1)\n",
    "\n",
    "    # Identify categorical columns\n",
    "    cat_cols = list(X_train.select_dtypes(include=['object', 'category']).columns)\n",
    "    \n",
    "    # Encoding Techniques\n",
    "    if encoding_technique == 'onehot':\n",
    "        X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "        X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "        X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "        \n",
    "        smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "    elif encoding_technique == 'ordinal':\n",
    "        X_train_encoded = X_train.copy()\n",
    "        X_test_encoded = X_test.copy()\n",
    "        ordinal_mappings = {}\n",
    "\n",
    "        for col in cat_cols:\n",
    "            categories = X_train_encoded[col].astype('category').cat.categories.tolist()\n",
    "            ordinal_mappings[col] = {val: idx for idx, val in enumerate(categories)}\n",
    "            X_train_encoded[col] = X_train_encoded[col].map(ordinal_mappings[col])\n",
    "            X_test_encoded[col] = X_test_encoded[col].map(ordinal_mappings[col])\n",
    "\n",
    "        smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "    elif encoding_technique == 'frequency':\n",
    "        freq_mappings = {col: X_train[col].value_counts(normalize=True).to_dict() for col in cat_cols}\n",
    "        X_train_encoded = X_train.copy()\n",
    "        X_test_encoded = X_test.copy()\n",
    "\n",
    "        for col in cat_cols:\n",
    "            X_train_encoded[col] = X_train_encoded[col].map(freq_mappings[col])\n",
    "            X_test_encoded[col] = X_test_encoded[col].map(freq_mappings[col])\n",
    "\n",
    "        smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid encoding technique. Choose 'onehot', 'ordinal', or 'frequency'.\")\n",
    "\n",
    "    # Convert categorical columns to numeric (Ensures no string values)\n",
    "    X_train_res = pd.DataFrame(X_train_res).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test_encoded = pd.DataFrame(X_test_encoded).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fit RandomForest Classifier\n",
    "    clf_smote = RandomForestClassifier(random_state=random_state)\n",
    "    clf_smote.fit(X_train_res, y_train_res)\n",
    "    y_pred_smote = clf_smote.predict(X_test_encoded)\n",
    "    f1_smote = f1_score(y_test, y_pred_smote, average='weighted')\n",
    "\n",
    "    # --- SMOTENC ---\n",
    "    X_train_smotenc = X_train.copy()\n",
    "    X_test_smotenc = X_test.copy()\n",
    "    for col in cat_cols:\n",
    "        X_train_smotenc[col] = X_train_smotenc[col].astype('category').cat.codes\n",
    "        X_test_smotenc[col] = X_test_smotenc[col].astype('category').cat.codes\n",
    "\n",
    "    categorical_indices = [X_train_smotenc.columns.get_loc(col) for col in cat_cols]\n",
    "\n",
    "    smotenc = SMOTENC(categorical_features=categorical_indices, random_state=random_state, k_neighbors=k_neighbors)\n",
    "    X_train_res_smotenc, y_train_res_smotenc = smotenc.fit_resample(X_train_smotenc, y_train)\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    X_train_res_smotenc = pd.DataFrame(X_train_res_smotenc).apply(pd.to_numeric, errors='coerce')\n",
    "    X_test_smotenc = pd.DataFrame(X_test_smotenc).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fit RandomForest Classifier\n",
    "    clf_smotenc = RandomForestClassifier(random_state=random_state)\n",
    "    clf_smotenc.fit(X_train_res_smotenc, y_train_res_smotenc)\n",
    "    y_pred_smotenc = clf_smotenc.predict(X_test_smotenc)\n",
    "    f1_smotenc = f1_score(y_test, y_pred_smotenc, average='weighted')\n",
    "\n",
    "    return f1_smote, f1_smotenc\n",
    "\n",
    "# Selected datasets\n",
    "selected = [\"flare1\", \"crx\", \"titanic\"]\n",
    "selected_datasets = {name: df for name, df in all_datasets.items() if name in selected}\n",
    "\n",
    "n_iter = 10\n",
    "results_f1 = {name: {\"smote\": [], \"smotenc\": []} for name in selected_datasets.keys()}\n",
    "\n",
    "encoding_techniques = [\"onehot\", \"ordinal\", \"frequency\"]\n",
    "\n",
    "# Loop over datasets and iterations\n",
    "for encoding_technique in encoding_techniques:\n",
    "    print(f\"\\nEvaluating with encoding technique: {encoding_technique}\")\n",
    "    for name, df in selected_datasets.items():\n",
    "        target = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Processing dataset: {name}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            rs = 42 + i\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, target, test_size=0.3, random_state=rs\n",
    "            )\n",
    "            scores = evaluate_models_on_split(X_train, X_test, y_train, y_test, encoding_technique, random_state=rs)\n",
    "            if scores[0] is not None and scores[1] is not None:\n",
    "                results_f1[name][\"smote\"].append(scores[0])\n",
    "                results_f1[name][\"smotenc\"].append(scores[1])\n",
    "\n",
    "        avg_smote = np.mean(results_f1[name][\"smote\"]) if results_f1[name][\"smote\"] else np.nan\n",
    "        avg_smotenc = np.mean(results_f1[name][\"smotenc\"]) if results_f1[name][\"smotenc\"] else np.nan\n",
    "        print(f\"Dataset: {name} - Avg Weighted F1 (SMOTE {encoding_technique}): {avg_smote:.4f}\")\n",
    "        print(f\"Dataset: {name} - Avg Weighted F1 (SMOTENC): {avg_smotenc:.4f}\")\n",
    "\n",
    "# Plot Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for encoding in encoding_techniques:\n",
    "    smote_scores = [np.mean(results_f1[d][\"smote\"]) for d in selected]\n",
    "    smotenc_scores = [np.mean(results_f1[d][\"smotenc\"]) for d in selected]\n",
    "    plt.bar(selected, smote_scores, label=f'SMOTE {encoding}', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.ylabel('Average Weighted F1 Score')\n",
    "plt.title('Comparison of SMOTE Encoding Techniques vs. SMOTENC')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
