{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericMethodHandler:\n",
    "    def __init__(self, dataset, target_column, test_size = 0.3, imbalance_ratio = 0.2, random_state=42):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.target_column = target_column\n",
    "        self.test_size = test_size\n",
    "        self.imbalance_ratio = imbalance_ratio\n",
    "        self.random_state = random_state\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "\n",
    "        # Split into features (X) and target (y)\n",
    "        x = self.dataset.drop(columns=[self.target_column])\n",
    "        y = self.dataset[self.target_column]\n",
    "\n",
    "        # Fill missing values for numerical columns\n",
    "        for col in x.select_dtypes(include=[\"float\", \"int\"]).columns:\n",
    "            x[col] = x[col].fillna(x[col].median())\n",
    "\n",
    "        # Fill missing values for categorical columns\n",
    "        for col in x.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
    "            x[col] = x[col].fillna(x[col].mode()[0])\n",
    "\n",
    "        # Encode categorical variables using one-hot encoding\n",
    "        x = pd.get_dummies(x, drop_first=True)\n",
    "\n",
    "        # Split into training and test sets\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y, test_size=self.test_size, random_state = self.random_state\n",
    "        )\n",
    "\n",
    "        # Introduce class imbalance in the training set\n",
    "        self.x_train, self.y_train = self._introduce_imbalance(x_train, y_train)\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "\n",
    "    def _introduce_imbalance(self, x_train, y_train):\n",
    "\n",
    "        train_df = pd.concat([x_train, y_train], axis=1)\n",
    "        majority_class = y_train.value_counts().idxmax()  # Identify the majority class\n",
    "        majority_samples = train_df[train_df[self.target_column] == majority_class]\n",
    "        minority_samples = train_df[train_df[self.target_column] != majority_class]\n",
    "\n",
    "        # Reduce the majority class\n",
    "        reduced_majority = majority_samples.sample(\n",
    "            frac=self.imbalance_ratio, random_state = self.random_state\n",
    "        )\n",
    "\n",
    "        # Combine reduced majority and minority samples\n",
    "        imbalanced_train_df = pd.concat([reduced_majority, minority_samples])\n",
    "        return imbalanced_train_df.drop(columns=[self.target_column]), imbalanced_train_df[self.target_column]\n",
    "\n",
    "    def apply_smote(self):\n",
    "\n",
    "        smote = SMOTE(random_state = self.random_state)\n",
    "        self.x_train, self.y_train = smote.fit_resample(self.x_train, self.y_train)\n",
    "\n",
    "    def apply_adasyn(self):\n",
    "\n",
    "        adasyn = ADASYN(random_state = self.random_state)\n",
    "        self.x_train, self.y_train = adasyn.fit_resample(self.x_train, self.y_train)\n",
    "    \n",
    "    def apply_random_undersampling(self):\n",
    "\n",
    "        rus = RandomUnderSampler(random_state = self.random_state)\n",
    "        self.x_train, self.y_train = rus.fit_resample(self.x_train, self.y_train)\n",
    "\n",
    "    def train_and_evaluate_generalized(self, method, max_depth = None, n_estimators = 10):\n",
    "\n",
    "        if method == \"none\":\n",
    "            model = RandomForestClassifier(max_depth=max_depth, random_state = self.random_state)\n",
    "            model.fit(self.x_train, self.y_train)\n",
    "            predictions = model.predict(self.x_test)\n",
    "            print(\"\\n--- No Method Results ---\")\n",
    "        \n",
    "        elif method == \"smote\":\n",
    "            self.apply_smote()\n",
    "            model = RandomForestClassifier(max_depth=max_depth, random_state = self.random_state)\n",
    "            model.fit(self.x_train, self.y_train)\n",
    "            predictions = model.predict(self.x_test)\n",
    "            print(\"\\n--- Smote Method Results ---\")\n",
    "        \n",
    "        elif method == \"class_weights\":\n",
    "            model = RandomForestClassifier(max_depth=max_depth, class_weight=\"balanced\", random_state = self.random_state)\n",
    "            model.fit(self.x_train, self.y_train)\n",
    "            predictions = model.predict(self.x_test)\n",
    "            print(\"\\n--- Class Weights Method Results ---\")\n",
    "\n",
    "        elif method == \"adasyn\":\n",
    "            self.apply_adasyn()\n",
    "            model = RandomForestClassifier(max_depth=max_depth, random_state = self.random_state)\n",
    "            model.fit(self.x_train, self.y_train)\n",
    "            predictions = model.predict(self.x_test)\n",
    "            print(\"\\n--- Adasyn Method Results ---\")\n",
    "        \n",
    "        elif method == \"random_undersampling\":\n",
    "            self.apply_random_undersampling()\n",
    "            model = RandomForestClassifier(max_depth=max_depth, random_state = self.random_state)\n",
    "            model.fit(self.x_train, self.y_train)\n",
    "            predictions = model.predict(self.x_test)\n",
    "            print(\"\\n--- Random Undersampling Method Results ---\")\n",
    "\n",
    "        elif method == \"easy_ensemble\":\n",
    "            model = EasyEnsembleClassifier(n_estimators=n_estimators, random_state = self.random_state)\n",
    "            model.fit(self.x_train, self.y_train)\n",
    "            predictions = model.predict(self.x_test)\n",
    "            print(\"\\n--- Easy Ensemble Method Results ---\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid Method\")\n",
    "\n",
    "        print(\"Accuracy: \", accuracy_score(self.y_test, predictions))\n",
    "        print(\"Classification Report: \\n\", classification_report(self.y_test, predictions))\n",
    "        print(\"Confusion Matrix: \\n\", confusion_matrix(self.y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "titanic_data = pd.read_csv(r\"C:\\Users\\potat\\OneDrive\\Desktop\\titanic.csv\")\n",
    "park_data = pd.read_csv(r\"C:\\Users\\potat\\OneDrive\\Desktop\\parkinsons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = GenericMethodHandler(\n",
    "    dataset = titanic_data,\n",
    "    target_column=\"Survived\",\n",
    "    test_size=0.3,\n",
    "    imbalance_ratio=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- No Method Results ---\n",
      "Accuracy:  0.6869158878504673\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.57      0.68       122\n",
      "           1       0.60      0.84      0.70        92\n",
      "\n",
      "    accuracy                           0.69       214\n",
      "   macro avg       0.71      0.71      0.69       214\n",
      "weighted avg       0.73      0.69      0.69       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[70 52]\n",
      " [15 77]]\n",
      "\n",
      "--- Smote Method Results ---\n",
      "Accuracy:  0.7523364485981309\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76       122\n",
      "           1       0.67      0.83      0.74        92\n",
      "\n",
      "    accuracy                           0.75       214\n",
      "   macro avg       0.76      0.76      0.75       214\n",
      "weighted avg       0.77      0.75      0.75       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[85 37]\n",
      " [16 76]]\n",
      "\n",
      "--- Class Weights Method Results ---\n",
      "Accuracy:  0.7523364485981309\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76       122\n",
      "           1       0.67      0.83      0.74        92\n",
      "\n",
      "    accuracy                           0.75       214\n",
      "   macro avg       0.76      0.76      0.75       214\n",
      "weighted avg       0.77      0.75      0.75       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[85 37]\n",
      " [16 76]]\n",
      "\n",
      "--- Adasyn Method Results ---\n",
      "Accuracy:  0.7523364485981309\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76       122\n",
      "           1       0.67      0.83      0.74        92\n",
      "\n",
      "    accuracy                           0.75       214\n",
      "   macro avg       0.76      0.76      0.75       214\n",
      "weighted avg       0.77      0.75      0.75       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[85 37]\n",
      " [16 76]]\n",
      "\n",
      "--- Random Undersampling Method Results ---\n",
      "Accuracy:  0.7429906542056075\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.67      0.75       122\n",
      "           1       0.66      0.84      0.74        92\n",
      "\n",
      "    accuracy                           0.74       214\n",
      "   macro avg       0.75      0.75      0.74       214\n",
      "weighted avg       0.76      0.74      0.74       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[82 40]\n",
      " [15 77]]\n",
      "\n",
      "--- Easy Ensemble Method Results ---\n",
      "Accuracy:  0.7616822429906542\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78       122\n",
      "           1       0.70      0.78      0.74        92\n",
      "\n",
      "    accuracy                           0.76       214\n",
      "   macro avg       0.76      0.76      0.76       214\n",
      "weighted avg       0.77      0.76      0.76       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[91 31]\n",
      " [20 72]]\n"
     ]
    }
   ],
   "source": [
    "handler.train_and_evaluate_generalized(method=\"none\")\n",
    "handler.train_and_evaluate_generalized(method=\"smote\")\n",
    "handler.train_and_evaluate_generalized(method=\"class_weights\")\n",
    "handler.train_and_evaluate_generalized(method=\"adasyn\")\n",
    "handler.train_and_evaluate_generalized(method=\"random_undersampling\")\n",
    "handler.train_and_evaluate_generalized(method=\"easy_ensemble\", n_estimators=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
