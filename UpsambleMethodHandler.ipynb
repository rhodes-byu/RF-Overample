{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericMethodHandler:\n",
    "    def __init__(self, dataset, target_column, test_size=0.3, imbalance_ratio=0.2, random_state=42):\n",
    "        self.dataset = dataset\n",
    "        self.target_column = target_column\n",
    "        self.test_size = test_size\n",
    "        self.imbalance_ratio = imbalance_ratio\n",
    "        self.random_state = random_state\n",
    "        self.original_x_train = None\n",
    "        self.original_y_train = None\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        # Split into features (X) and target (y)\n",
    "        x = self.dataset.drop(columns=[self.target_column])\n",
    "        y = self.dataset[self.target_column]\n",
    "\n",
    "        # Fill missing values for numerical columns\n",
    "        for col in x.select_dtypes(include=[\"float\", \"int\"]).columns:\n",
    "            x[col] = x[col].fillna(x[col].median())\n",
    "\n",
    "        # Fill missing values for categorical columns\n",
    "        for col in x.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
    "            x[col] = x[col].fillna(x[col].mode()[0])\n",
    "\n",
    "        # Encode categorical variables\n",
    "        x = pd.get_dummies(x, drop_first=True)\n",
    "\n",
    "        # Split into training and test sets\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y, test_size=self.test_size, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # Introduce class imbalance in the training set\n",
    "        self.original_x_train, self.original_y_train = self._introduce_imbalance(x_train, y_train)\n",
    "        self.x_train, self.y_train = self.original_x_train.copy(), self.original_y_train.copy()\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "\n",
    "    def _introduce_imbalance(self, x_train, y_train):\n",
    "        # Create an imbalanced dataset\n",
    "        train_df = pd.concat([x_train, y_train], axis=1)\n",
    "        majority_class = y_train.value_counts().idxmax()\n",
    "        majority_samples = train_df[train_df[self.target_column] == majority_class]\n",
    "        minority_samples = train_df[train_df[self.target_column] != majority_class]\n",
    "\n",
    "        # Reduce the majority class\n",
    "        reduced_majority = majority_samples.sample(\n",
    "            frac=self.imbalance_ratio, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # Combine reduced majority and minority samples\n",
    "        imbalanced_train_df = pd.concat([reduced_majority, minority_samples])\n",
    "        return imbalanced_train_df.drop(columns=[self.target_column]), imbalanced_train_df[self.target_column]\n",
    "\n",
    "    def reset_training_data(self):\n",
    "        # Reset the training data to the original state\n",
    "        self.x_train, self.y_train = self.original_x_train.copy(), self.original_y_train.copy()\n",
    "\n",
    "    def visualize_class_distribution(self, dataset=\"train\"):\n",
    "        # Visualize class distribution for training or test set\n",
    "        if dataset == \"train\":\n",
    "            sns.countplot(x=self.y_train)\n",
    "            plt.title(\"Class Distribution (Train Set)\")\n",
    "            plt.show()\n",
    "        elif dataset == \"test\":\n",
    "            sns.countplot(x=self.y_test)\n",
    "            plt.title(\"Class Distribution (Test Set)\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise ValueError(\"Dataset must be 'train' or 'test'\")\n",
    "\n",
    "    def apply_smote(self, sampling_strategy=\"auto\"):\n",
    "        smote = SMOTE(sampling_strategy=sampling_strategy, random_state=self.random_state)\n",
    "        self.x_train, self.y_train = smote.fit_resample(self.x_train, self.y_train)\n",
    "\n",
    "    def apply_adasyn(self, sampling_strategy=\"auto\"):\n",
    "        adasyn = ADASYN(sampling_strategy=sampling_strategy, random_state=self.random_state)\n",
    "        self.x_train, self.y_train = adasyn.fit_resample(self.x_train, self.y_train)\n",
    "\n",
    "    def apply_random_undersampling(self, sampling_strategy=\"auto\"):\n",
    "        rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=self.random_state)\n",
    "        self.x_train, self.y_train = rus.fit_resample(self.x_train, self.y_train)\n",
    "\n",
    "    def train_and_evaluate_generalized(self, method, max_depth=None, n_estimators=10):\n",
    "        self.reset_training_data()  # Reset training data before applying a method\n",
    "        if method == \"none\":\n",
    "            model = RandomForestClassifier(max_depth=max_depth, random_state=self.random_state)\n",
    "        elif method == \"class_weights\":\n",
    "            model = RandomForestClassifier(\n",
    "                max_depth=max_depth, class_weight=\"balanced\", random_state=self.random_state\n",
    "            )\n",
    "        elif method == \"smote\":\n",
    "            self.apply_smote()\n",
    "            model = RandomForestClassifier(max_depth=max_depth, random_state=self.random_state)\n",
    "        elif method == \"adasyn\":\n",
    "            self.apply_adasyn()\n",
    "            model = RandomForestClassifier(max_depth=max_depth, random_state=self.random_state)\n",
    "        elif method == \"random_undersampling\":\n",
    "            self.apply_random_undersampling()\n",
    "            model = RandomForestClassifier(max_depth=max_depth, random_state=self.random_state)\n",
    "        elif method == \"easy_ensemble\":\n",
    "            model = EasyEnsembleClassifier(n_estimators=n_estimators, random_state=self.random_state)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Method\")\n",
    "\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        predictions = model.predict(self.x_test)\n",
    "\n",
    "        print(f\"\\n--- {method.capitalize()} Method Results ---\")\n",
    "        print(\"Accuracy: \", accuracy_score(self.y_test, predictions))\n",
    "        print(\"Classification Report: \\n\", classification_report(self.y_test, predictions))\n",
    "        print(\"Confusion Matrix: \\n\", confusion_matrix(self.y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets (Change loation with local computer location)\n",
    "titanic_data = pd.read_csv(r\"C:\\Users\\potat\\OneDrive\\Desktop\\titanic.csv\")\n",
    "park_data = pd.read_csv(r\"C:\\Users\\potat\\OneDrive\\Desktop\\parkinsons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler1 = GenericMethodHandler(\n",
    "    dataset = titanic_data,\n",
    "    target_column=\"Survived\",\n",
    "    test_size=0.3,\n",
    "    imbalance_ratio=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "handler1.visualize_class_distribution(\"train\")\n",
    "handler1.visualize_class_distribution(\"test\")\n",
    "\n",
    "handler1.train_and_evaluate_generalized(method=\"none\")\n",
    "handler1.train_and_evaluate_generalized(method=\"smote\")\n",
    "handler1.train_and_evaluate_generalized(method=\"class_weights\")\n",
    "handler1.train_and_evaluate_generalized(method=\"adasyn\")\n",
    "handler1.train_and_evaluate_generalized(method=\"random_undersampling\")\n",
    "handler1.train_and_evaluate_generalized(method=\"easy_ensemble\", n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- No Method Results ---\n",
      "Accuracy:  0.6869158878504673\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.57      0.68       122\n",
      "           1       0.60      0.84      0.70        92\n",
      "\n",
      "    accuracy                           0.69       214\n",
      "   macro avg       0.71      0.71      0.69       214\n",
      "weighted avg       0.73      0.69      0.69       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[70 52]\n",
      " [15 77]]\n",
      "\n",
      "--- Smote Method Results ---\n",
      "Accuracy:  0.7523364485981309\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76       122\n",
      "           1       0.67      0.83      0.74        92\n",
      "\n",
      "    accuracy                           0.75       214\n",
      "   macro avg       0.76      0.76      0.75       214\n",
      "weighted avg       0.77      0.75      0.75       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[85 37]\n",
      " [16 76]]\n",
      "\n",
      "--- Class Weights Method Results ---\n",
      "Accuracy:  0.7523364485981309\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76       122\n",
      "           1       0.67      0.83      0.74        92\n",
      "\n",
      "    accuracy                           0.75       214\n",
      "   macro avg       0.76      0.76      0.75       214\n",
      "weighted avg       0.77      0.75      0.75       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[85 37]\n",
      " [16 76]]\n",
      "\n",
      "--- Adasyn Method Results ---\n",
      "Accuracy:  0.7523364485981309\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76       122\n",
      "           1       0.67      0.83      0.74        92\n",
      "\n",
      "    accuracy                           0.75       214\n",
      "   macro avg       0.76      0.76      0.75       214\n",
      "weighted avg       0.77      0.75      0.75       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[85 37]\n",
      " [16 76]]\n",
      "\n",
      "--- Random Undersampling Method Results ---\n",
      "Accuracy:  0.7429906542056075\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.67      0.75       122\n",
      "           1       0.66      0.84      0.74        92\n",
      "\n",
      "    accuracy                           0.74       214\n",
      "   macro avg       0.75      0.75      0.74       214\n",
      "weighted avg       0.76      0.74      0.74       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[82 40]\n",
      " [15 77]]\n",
      "\n",
      "--- Easy Ensemble Method Results ---\n",
      "Accuracy:  0.7616822429906542\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78       122\n",
      "           1       0.70      0.78      0.74        92\n",
      "\n",
      "    accuracy                           0.76       214\n",
      "   macro avg       0.76      0.76      0.76       214\n",
      "weighted avg       0.77      0.76      0.76       214\n",
      "\n",
      "Confusion Matrix: \n",
      " [[91 31]\n",
      " [20 72]]\n"
     ]
    }
   ],
   "source": [
    "handler2 = GenericMethodHandler(\n",
    "    dataset = park_data,\n",
    "    target_column=\"status\",\n",
    "    test_size=0.3,\n",
    "    imbalance_ratio=0.2\n",
    ")\n",
    "\n",
    "handler2.visualize_class_distribution(\"train\")\n",
    "handler2.visualize_class_distribution(\"test\")\n",
    "\n",
    "handler2.train_and_evaluate_generalized(method=\"none\")\n",
    "handler2.train_and_evaluate_generalized(method=\"smote\")\n",
    "handler2.train_and_evaluate_generalized(method=\"class_weights\")\n",
    "handler2.train_and_evaluate_generalized(method=\"adasyn\")\n",
    "handler2.train_and_evaluate_generalized(method=\"random_undersampling\")\n",
    "handler2.train_and_evaluate_generalized(method=\"easy_ensemble\", n_estimators=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
